# train_light.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms, datasets
import matplotlib.pyplot as plt
import time
import os
from light_model import LightCatDogCNN
from utils import plot_training_history, save_checkpoint

# è®¾ç½®è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# è¶…å‚æ•°é…ç½®
config = {
    'batch_size': 32,
    'learning_rate': 0.001,
    'epochs': 30,
    'patience': 7,
    'weight_decay': 1e-4
}

def train_light_model():
    # æ•°æ®é¢„å¤„ç†
    train_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(10),
        transforms.ColorJitter(brightness=0.2, contrast=0.2),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # åŠ è½½æ•°æ®
    train_dataset = datasets.ImageFolder('./data/train', transform=train_transform)
    val_dataset = datasets.ImageFolder('./data/val', transform=val_transform)
    
    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2)
    
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}")
    print(f"éªŒè¯æ ·æœ¬æ•°: {len(val_dataset)}")
    print(f"ç±»åˆ«æ˜ å°„: {train_dataset.class_to_idx}")
    
    # ä½¿ç”¨è½»é‡æ¨¡å‹
    model = LightCatDogCNN(num_classes=2, dropout_rate=0.5)
    model = model.to(device)
    
    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), 
                          lr=config['learning_rate'], 
                          weight_decay=config['weight_decay'])
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)
    
    # è®­ç»ƒå†å²è®°å½•
    train_losses = []
    train_accs = []
    val_losses = []
    val_accs = []
    
    best_val_acc = 0.0
    patience_counter = 0
    initial_lr = config['learning_rate']
    
    print("å¼€å§‹è®­ç»ƒè½»é‡æ¨¡å‹...")
    
    for epoch in range(config['epochs']):
        start_time = time.time()
        
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        running_loss = 0.0
        running_corrects = 0
        
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            _, preds = torch.max(outputs, 1)
            running_loss += loss.item() * inputs.size(0)
            running_corrects += torch.sum(preds == labels.data)
        
        epoch_loss = running_loss / len(train_dataset)
        epoch_acc = running_corrects.double() / len(train_dataset)
        
        train_losses.append(epoch_loss)
        train_accs.append(epoch_acc.cpu())
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_running_loss = 0.0
        val_running_corrects = 0
        
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                _, preds = torch.max(outputs, 1)
                
                val_running_loss += loss.item() * inputs.size(0)
                val_running_corrects += torch.sum(preds == labels.data)
        
        val_epoch_loss = val_running_loss / len(val_dataset)
        val_epoch_acc = val_running_corrects.double() / len(val_dataset)
        
        val_losses.append(val_epoch_loss)
        val_accs.append(val_epoch_acc.cpu())
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step(val_epoch_loss)
        
        # æ‰‹åŠ¨æ‰“å°å­¦ä¹ ç‡å˜åŒ–
        current_lr = optimizer.param_groups[0]['lr']
        if current_lr != initial_lr:
            print(f"  å­¦ä¹ ç‡æ›´æ–°ä¸º: {current_lr:.6f}")
            initial_lr = current_lr
        
        epoch_time = time.time() - start_time
        
        print(f'Epoch {epoch+1}/{config["epochs"]} ({epoch_time:.1f}s):')
        print(f'  è®­ç»ƒæŸå¤±: {epoch_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {epoch_acc:.4f}')
        print(f'  éªŒè¯æŸå¤±: {val_epoch_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_epoch_acc:.4f}')
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_epoch_acc > best_val_acc:
            best_val_acc = val_epoch_acc
            patience_counter = 0
            save_checkpoint({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'best_val_acc': best_val_acc,
                'config': config,
                'class_to_idx': train_dataset.class_to_idx
            }, 'light_best_model.pth')
            print(f'  âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}')
        else:
            patience_counter += 1
        
        # æ—©åœæ£€æŸ¥
        if patience_counter >= config['patience']:
            print(f'ğŸ›‘ æ—©åœï¼åœ¨ epoch {epoch+1} åœæ­¢è®­ç»ƒ')
            break
    
    # ç»˜åˆ¶è®­ç»ƒå†å²
    plot_training_history(train_losses, val_losses, train_accs, val_accs)
    
    print(f"è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
    
    return model

if __name__ == "__main__":
    model = train_light_model()